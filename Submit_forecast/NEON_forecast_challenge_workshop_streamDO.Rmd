---
title: "NEON forecast challengeworkshop stream DO"
author: "Kelly- adapted from Freya Olsson"
output:
  md_document: 
    variant: markdown_github
    number_sections: true
    toc: true
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


The following code chunk should be run to install packages.

```{r eval = F}
install.packages('remotes')
install.packages('fpp3') # package for applying simple forecasting methods
install.packages('tsibble') # package for dealing with time series data sets and tsibble objects
install.packages('tidyverse') # collection of R packages for data manipulation, analysis, and visualisation
install.packages('lubridate') # working with dates and times
remotes::install_github('eco4cast/neon4cast') # package from NEON4cast challenge organisers to assist with forecast building and submission
```

```{r}
version$version.string

library(tidyverse)
library(lubridate)
library(readr)
```

## Goals: 
What: Freshwater surface water temperature, oxygen, and chlorophyll-a.

Where: 27 river/stream NEON sites.

When: Daily forecasts for at least 30-days in the future. New forecast submissions, that use new data to update the forecast, are accepted daily. The only requirement is that submissions are predictions of the future at the time the forecast is submitted.

## Submission requirements
For an ensemble forecast, the `family` column uses the word `ensemble` to designate that it is a ensemble forecast and the parameter column is the ensemble member number (1, 2, 3 ...). For a distribution forecast, the `family` column uses the word `normal` to designate a normal distribution and the parameter column must have the words mu and sigma for each forecasted variable, site_id, and datetime. For forecasts that don't have a normal distribution we recommend using the ensemble format and sampling from your non-normal distribution to generate a set of ensemble members that represents your distribution. I will go through examples of both `ensemble` and `normal` forecasts as examples.

# The forecasting workflow

## Read in the data

We start forecasting by first looking at the historic data - called the 'targets'. These data are available near real-time, with the latency of approximately 24-48 hrs. Here is how you read in the data from the targets file available from the EFI server.

```{r eval=TRUE, echo = TRUE, error=FALSE, warning=FALSE, message=FALSE}
#read in the targets data
targets <- read_csv('https://data.ecoforecast.org/neon4cast-targets/aquatics/aquatics-targets.csv.gz')
```

Information on the NEON sites can be found in the `NEON_Field_Site_Metadata_20220412.csv` file on GitHub. It can be filtered to only include aquatic sites. This table has information about the field sites, including location, ecoregion, information about the watershed (e.g. elevation, mean annual precipitation and temperature), and lake depth.

```{r eval=TRUE, echo = TRUE, error=FALSE, warning=FALSE, message=FALSE}
# read in the sites data
aquatic_sites <- read_csv("https://raw.githubusercontent.com/eco4cast/neon4cast-targets/main/NEON_Field_Site_Metadata_20220412.csv") |>
  dplyr::filter(aquatics == 1)
```

```{r eval = T, echo = F}
targets[1000:1010,]

```

```{r}
stream_sites <- aquatic_sites %>%
  filter(field_site_subtype == "Wadeable Stream")

unique(stream_sites$field_site_id)

targets <- targets %>%
  filter(site_id %in% stream_sites$field_site_id)
```

## Visualize the data

```{r eval = T, echo = F, warning=FALSE, fig.dim=c(10,10), fig.cap=c('Figure: Temperature targets data at aquatic sites provided by EFI for the NEON forecasting challgenge', 'Figure: Oxygen targets data at aquatic sites provided by EFI for the NEON forecasting challgenge', 'Figure: Chlorophyll targets data at aquatic sites provided by EFI for the NEON forecasting challgenge. Chlorophyll data is only available at lake and river sites')}
targets %>%
  filter(variable == 'temperature') %>%
  ggplot(., aes(x = datetime, y = observation)) +
  geom_point() +
  theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  facet_wrap(~site_id, scales = 'free_y') +
  labs(title = 'temperature')

targets %>%
  filter(variable == 'oxygen') %>%
  ggplot(., aes(x = datetime, y = observation)) +
  geom_point() +  
  theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  facet_wrap(~site_id, scales = 'free_y')+
  labs(title = 'oxygen')

```

```{r}
targets <- targets %>%
  filter(variable == 'oxygen')
```

# Introducing co-variates

One important step to overcome when thinking about generating forecasts is to include co-variates in the model. A water temperature forecast, for example, may be benefit from information about past and future weather. The neon4cast challenge package includes functions for downloading past and future NOAA weather forecasts for all of the NEON sites. The 3 types of data are as follows:

-   stage_1: raw forecasts - 31 member ensemble forecasts at 3 hr intervals for the first 10 days, and 6 hr intervals for up to 35 days at the NEON sites.
-   stage_2: a processed version of Stage 1 in which fluxes are standardized to per second rates, fluxes and states are interpolated to 1 hour intervals and variables are renamed to match conventions. We recommend this for obtaining future weather. Future weather forecasts include a 30-member ensemble of equally likely future weather conditions.
-   stage_3: can be viewed as the "historical" weather and is combination of day 1 weather forecasts (i.e., when the forecasts are most accurate).

This code create a connection to the dataset hosted on the eco4cast server (`neon4cast-drivers/noaa/gefs-v12`) using `arrow` functions. To download the data you have to tell the function to `collect()` it. These data set can be subsetted and filtered using `dplyr` functions prior to download to limit the memory usage.

You can read more about the NOAA forecasts available for the NEON sites [here:](https://projects.ecoforecast.org/neon4cast-docs/Shared-Forecast-Drivers.html)

## Download co-variates

### Download historic data

We will generate a water temperature forecast using `air_temperature` as a co-variate. Note: This code chunk can take a few minutes to execute as it accesses the NOAA data.

```{r, message=FALSE}
# past stacked weather
noaa_past_s3 <- neon4cast::noaa_stage3()

variables <- c("air_temperature", "air_pressure", "precipitation_flux") # precipitation
#Other variable names can be found at https://projects.ecoforecast.org/neon4cast-docs/Shared-Forecast-Drivers.html#stage-3

noaa_past <- noaa_past_s3  |> 
  dplyr::filter(site_id %in% stream_sites$field_site_id,
                datetime >= ymd('2017-01-01'),
                variable %in% variables) |> 
  dplyr::collect()

noaa_past
```

This is a stacked ensemble forecast of the one day ahead forecasts. To get an estimate of the historic conditions we can take a mean of these ensembles. We will also need to convert the temperatures to Celsius from Kelvin.

```{r}
# aggregate the past to mean values
noaa_past_mean <- noaa_past |> 
  mutate(datetime = as_date(datetime)) |> 
  group_by(datetime, site_id, variable) |> 
  summarize(prediction = mean(prediction, na.rm = TRUE), .groups = "drop") |> 
  pivot_wider(names_from = variable, values_from = prediction) |> 
  # convert air temp to C
  mutate(air_temperature = air_temperature - 273.15)
```

We can then look at the future weather forecasts in the same way but using the `noaa_stage2()`. The forecast becomes available from NOAA at 5am UTC the following day, so we take the air temperature forecast from yesterday (`noaa_date`) to make the water quality forecasts. Then we can use the ensembles to produce uncertainty in the water temperature forecast by forecasting multiple (31) future water temperatures.

### Download future weather forecasts

```{r, message=FALSE}
# New forecast only available at 5am UTC the next day

forecast_date <- Sys.Date() 
noaa_date <- forecast_date - days(1)

noaa_future_s3 <- neon4cast::noaa_stage2(start_date = as.character(noaa_date))
#variables <- c("air_temperature")

noaa_future <- noaa_future_s3 |> 
  dplyr::filter(datetime >= forecast_date,
                site_id %in% lake_sites$field_site_id,
                variable %in% variables) |> 
  collect()

```

The forecasts are hourly and we are interested in using daily mean air temperature for water temperature forecast generation.

```{r warning=F}
noaa_future_daily <- noaa_future |> 
  mutate(datetime = as_date(datetime)) |> 
  # mean daily forecasts at each site per ensemble
  group_by(datetime, site_id, parameter, variable) |> 
  summarize(prediction = mean(prediction)) |>
  pivot_wider(names_from = variable, values_from = prediction) |>
  # convert to Celsius
  mutate(air_temperature = air_temperature - 273.15) |> 
  select(datetime, site_id, air_temperature, precipitation_flux, air_pressure, parameter)

noaa_future_daily
```

Now we have a timeseries of historic data and a 30 member ensemble forecast of future air temperatures

```{r echo = F, fig.cap = c('Figure: historic and future NOAA air temeprature forecasts at lake sites', 'Figure: last two months of historic air temperature forecasts and 35 day ahead forecast')}
ggplot(noaa_future_daily, aes(x=datetime, y=air_temperature)) +
  geom_line(aes(group = parameter), alpha = 0.4)+
  geom_line(data = noaa_past_mean, colour = 'darkblue') +
  facet_wrap(~site_id, scales = 'free')

ggplot(noaa_future_daily, aes(x=datetime, y=air_temperature)) +
  geom_line(aes(group = parameter), alpha = 0.4)+
  geom_line(data = noaa_past_mean, colour = 'darkblue') +
  coord_cartesian(xlim = c(noaa_date - days(60),
                           noaa_date + days(35)))+
  facet_wrap(~site_id, scales = 'free')

ggplot(noaa_future_daily, aes(x=datetime, y=precipitation_flux)) +
  geom_line(aes(group = parameter), alpha = 0.4)+
  geom_line(data = noaa_past_mean, colour = 'darkblue') +
  coord_cartesian(xlim = c(noaa_date - days(60),
                           noaa_date + days(35)))+
  facet_wrap(~site_id, scales = 'free')

ggplot(noaa_future_daily, aes(x=datetime, y=air_pressure)) +
  geom_line(aes(group = parameter), alpha = 0.4)+
  geom_line(data = noaa_past_mean, colour = 'darkblue') +
  coord_cartesian(xlim = c(noaa_date - days(60),
                           noaa_date + days(35)))+
  facet_wrap(~site_id, scales = 'free')

```

# Model 1: Linear model with covariates

We will fit a simple linear model between historic air temperature and the water temperature targets data. Using this model we can then use our future estimates of air temperature (all 30 ensembles) to estimate water temperature at each site. The ensemble weather forecast will therefore propagate uncertainty into the water temperature forecast and give an estimate of driving data uncertainty.

We will start by joining the historic weather data with the targets to aid in fitting the linear model.

```{r}
targets_lm <- targets |> 
  filter(variable == 'oxygen') |>
  pivot_wider(names_from = 'variable', values_from = 'observation') |> 
  left_join(noaa_past_mean, 
            by = c("datetime","site_id"))

targets_lm[1000:1010,]
```

To fit the linear model we use the base R `lm()` but there are also methods to fit linear (and non-linear) models in the `fable::` package. You can explore the [documentation](https://otexts.com/fpp3/regression.html) for more information on the `fable::TSLM()` function. We can fit a separate linear model for each site. For example, at Lake Suggs, this would look like:

```{r, eval = F}
example_site <- 'LEWI'

site_target <- targets_lm |> 
  filter(site_id == example_site)

noaa_future_site <- noaa_future_daily |> 
  filter(site_id == example_site)

#Fit linear model based on past data: water temperature = m * air temperature + b
fit <- lm((site_target$oxygen) ~ site_target$air_temperature + site_target$air_pressure + site_target$precipitation_flux)
    
# use linear regression to forecast water temperature for each ensemble member
forecasted_O <- fit$coefficients[1] + fit$coefficients[2] * noaa_future_site$air_temperature +  fit$coefficients[3] * noaa_future_site$air_pressure + fit$coefficients[4] * noaa_future_site$precipitation_flux

```

We can loop through this for each site to create a site-wise forecast of water temperature based on a linear model and each forecasted air temperature. We can run this forecast for each site and then bind them together to submit as one forecast.

## Specify forecast model

```{r}
DO_lm_forecast <- NULL

for(i in 1:length(stream_sites$field_site_id)) {  
  
  example_site <- stream_sites$field_site_id[i]
  
  site_target <- targets_lm |>
    filter(site_id == example_site)

  noaa_future_site <- noaa_future_daily |> 
    filter(site_id == example_site)
  
  #Fit linear model based on past data: water temperature = m * air temperature + b
  #fit <- lm(site_target$oxygen ~ site_target$air_temperature)
  # fit <- lm(site_target$temperature ~ ....)
    
  # use linear regression to forecast water temperature for each ensemble member
  #forecasted_O <- fit$coefficients[1] + fit$coefficients[2] * noaa_future_site$air_temperature
    
  # put all the relavent information into a tibble that we can bind together
  DO <- tibble(datetime = noaa_future_site$datetime,
                        site_id = example_site,
                        parameter = noaa_future_site$parameter,
                        prediction = forecasted_O,
                        variable = "oxygen")
  
  DO_lm_forecast <- dplyr::bind_rows(DO_lm_forecast, DO)
  message(example_site, 'DO forecast run')
  
}
```

We now have 30 possible forecasts of water temperature at each site and each day. On this plot each line represents one of the possible forecasts and the range of forecasted water temperature is a simple quantification of the uncertainty in our forecast.

Looking back at the forecasts we produced:

```{r, fig.dim=c(30,35), echo = F, warning = F}
DO_lm_forecast %>% 
  filter(variable == 'oxygen') %>%
  ggplot(.,aes(x=datetime, y=(prediction), group = parameter)) + 
  geom_point(data = targets,aes(x=datetime, y=(observation), group = 'obs'), colour = 'darkblue') +
  geom_line(alpha = 0.3, aes(colour = 'ensemble member (parameter)')) + 
  facet_wrap(~site_id, scales = 'free_y') +
  scale_x_date(expand = c(0,0), date_labels = "%d %b") +
  labs(y = 'value') +
  geom_vline(aes(linetype = 'reference_datetime', xintercept = Sys.Date()), colour = 'blue', size = 1.5) +
  labs(title = 'site_id', subtitle = 'variable = oxygen', caption = 'prediction') + 
  annotate("text", x = Sys.Date() - days(10), y = 20, label = "past")  +
  annotate("text", x = Sys.Date() + days(12), y = 20, label = "future")  +
  theme_bw() +
  coord_cartesian(xlim = c(min(DO_lm_forecast$datetime) - 15,
                           Sys.Date() + 30)) +
  scale_linetype_manual(values = 'dashed', name = '') +
  scale_colour_manual(values = 'darkgrey', name = '') +
  theme(strip.text = element_text(colour = 'orange'),
        axis.title.y = element_text(colour = 'green'),
        axis.title.x = element_text(colour = 'red'),
        axis.text.y = element_text(colour = 'purple'),
        axis.text.x = element_text(colour = 'red'),
        plot.caption = element_text(hjust = 0, colour = 'purple'),
        plot.title = element_text(colour = 'orange'), 
        plot.subtitle = element_text(colour = 'green')) 
```

## Convert to EFI standard for submission

For an ensemble forecast the documentation specifies the following columns:

-   `datetime`: forecast timestamp for each time step
-   `reference_datetime`: The start of the forecast; this should be 0 times steps in the future. This should only be one value of reference_datetime in the file
-   `site_id`: NEON code for site
-   `family`: name of probability distribution that is described by the parameter values in the parameter column; only `normal` or `ensemble` are currently allowed.
-   `parameter`: integer value for forecast replicate (from the `.rep` in fable output);
-   `variable`: standardized variable name from the theme
-   `prediction`: forecasted value (from the `.sim` column in fable output)
-   `model_id`: model name (no spaces)

We need to make sure the dataframe is in the correct format and then we can submit this to the challenge as well! This is an ensemble forecast (specified in the `family` column).

```{r}
# Remember to change the model_id when you make changes to the model structure!
my_model_id <- 'StreamDO_lm'

DO_lm_forecast_EFI <- DO_lm_forecast %>%
  mutate(model_id = my_model_id,
         reference_datetime = as_date(min(datetime)) - days(1),
         family = 'ensemble',
         parameter = as.character(parameter)) %>%
  select(datetime, reference_datetime, site_id, family, parameter, variable, prediction, model_id)
```

## Submit forecast

Files need to be in the correct format for submission. The forecast organizers have created tools to help aid in the submission process. These tools can be downloaded from Github using `remotes::install_github(eco4cast/neon4cast)`. These include functions for submitting, scoring and reading forecasts:

-   `submit()` - submit the forecast file to the neon4cast server where it will be scored
-   `forecast_output_validator()` - will check the file is in the correct format to be submitted
-   `check_submission()` - check that your submission has been uploaded to the server

The file name needs to be in the format theme-reference_datetime-model_id

```{r eval = T}
# Start by writing the forecast to file
theme <- 'aquatics'
date <- DO_lm_forecast_EFI$reference_datetime[1]
forecast_name_1 <- paste0(DO_lm_forecast_EFI$model_id[1], ".csv")
forecast_file_1 <- paste(theme, date, forecast_name_1, sep = '-')
forecast_file_1


if (!dir.exists('Forecasts')) {
  dir.create('Forecasts')
}

write_csv(DO_lm_forecast_EFI, file.path('Forecasts',forecast_file_1))

neon4cast::forecast_output_validator(file.path('Forecasts',forecast_file_1))

```

### PAUSE if you want to submit

```{r eval = FALSE}
# can uses the neon4cast::forecast_output_validator() to check the forecast is in the right format
neon4cast::submit(forecast_file = file.path('Forecasts', forecast_file_1),
                  ask = T) # if ask = T (default), it will produce a pop-up box asking if you want to submit
```

Is the linear model a reasonable relationship between air temperature and water temperature? Would some non-linear relationship be better? What about using yesterday's air and water temperatures to predict tomorrow? Or including additional parameters? There's a lot of variability in water temperatures unexplained by air temperature alone.

```{r, echo=F, warning=F}
ggplot(targets_lm, aes(x=air_temperature, y= oxygen, colour = site_id)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = 'lm') +
  theme_bw()
```
# Alternative forecasting approaches
## Model 2: Persistence

This forecasting method uses a method from the `fable` R package which is installed via `fpp3` package. The `fable` package implements a range of different forecasting methods including Persistence Models, Moving Average Models, ARIMA Models and Time Series Models. The package integrates with `tidyverse` syntax and has good documentation and examples found [here](https://otexts.com/fpp3/).
`fable` and `fabletools`, are installed as part of the `fpp3` package and produce and deal with `mable` (model table) and `fable` (forecast table) objects. We will also use the `tidyverse` to manipulate and visualise the target data and forecasts. 

```{r, 'load packages', eval=TRUE, echo = TRUE, error=FALSE, warning=FALSE, message=FALSE}
install.packages('fpp3') # package for applying simple forecasting methods
install.packages('tsibble') # package for dealing with time series data sets and tsibble objects

#remotes::install_github("tidyverts/tsibble")

library(fpp3)      # package for forecasting
library(tsibble)   # package for dealing with time series data sets

library(neon4cast) 

# suppreses dplyr's summarise message
options(dplyr.summarise.inform = FALSE)
```



`fable` has some simple models that can be fitted to the target data. `fable` models require data to be in a tidy `tsibble` format. Tools for dealing with tsibble objects are found in the `tsibble` package. `tsibble` objects are similar in structure to `tibble` objects but have a built in timeseries element. This is specified in their creation as the  `index` or time variable. You also need to give the tsibble a `key`, which in combination with the index will uniquely identify each record. In our case the key variables will be `site_id` and `variable`. These models also require explicit gaps to be added for missing values (`fill_gaps()` will do this!).

For Random Walk (RW) forecasts (i.e., persistence), we simply set the forecast value be the value of the last observation. Start by reading in the observations. We will look at water temperature to start with.

The model would be specified and then the forecast generated as follows (using `fable` and `fabletools` functions). 

To specify the model we use `model(RW = RW(observation))` and then the forecast can be created by piping this model to the `generate()` function. `h = 30` tells the model how far in the future to forecast, and `bootstrap = TRUE` tells the function to generate multiple uncertainty through bootstrapping. Bootstrapping is where we run the forecast multiple times with a random error term drawn from the residuals of the fitted model. Doing this repeatedly, we obtain many possible futures (an ensemble forecast). We decide how many of these possibilities to simulate with the `times =...` argument.


```{r, eval = T}
targets_subset <- targets %>%
  filter(site_id == 'LEWI') 

RW_model <- targets_subset %>% 
  # fable requires a tsibble object with explicit gaps
  tsibble::as_tsibble(key = c('variable', 'site_id'), index = 'datetime') %>%
  tsibble::fill_gaps() %>%
  model(RW = RW(observation))

RW_daily_forecast <- RW_model %>% 
  generate(h = 30, bootstrap = T, times = 50)

```

The forecast produces an error ('the first lag window for simulations must be within the model's training set'). Why would that be? If you look at the targets data we can see that yesterday did not have an observation, so the model cannot produce a persistence forecast. 

```{r}
targets |> 
  filter(site_id == 'LEWI') |> 
  summarise(last_ob = max(datetime)) |> 
  pull()
```

Specifying the model and forecasts in this way would be fine if we have data observed to yesterday but this often isn't the case. For the NEON data the usual data latency is between 24-72 hours. This is obviously not ideal, but we need to think of a way to produce forecasts with the data we have. Instead we can start the forecast at the last observation, rather than yesterday. The forecast is run from that observation through today and then for 30 days in the future. In practice this means telling fable to forecasts 30 days + the number of days since last observation and then trimming the days that are in the past, before submitting the forecast.

We calculate the `reference_datetime` (starting data) and total `horizon` for each `site_id` and `variable` combination (here just temperature).  

```{r message=F}
# For the RW model need to start the forecast at the last non-NA day and then run to 30 days in the future
forecast_starts <- targets %>%
  filter(!is.na(observation)) %>%
  group_by(site_id, variable) %>%
  # Start the day after the most recent non-NA value
  summarise(reference_datetime = max(datetime) + 1) %>% # Date 
  mutate(h = (Sys.Date() - reference_datetime) + 30) %>% # Horizon value 
  ungroup() 

forecast_starts
```

You can see that the sites have different start dates, based on when the last observation was taken. We want to fit each site (and variable) model separately depending on its start date and calculated horizon. To do this I have written a custom function that runs the RW forecast. Within this function we:

* Tidy: Takes the targets and fills with NAs, and filters up to the last non-NA value. The data must have explicit gaps for the full time series and must be in a tsibble format to run `fable`. Every time step up to the start of the forecast must exist even if it is filled with NAs (except the day before the forecast starts)!
* Specify: Fits the RW model. We can also specify transformations to use within the model.  The `fable` package will automatically back-transform common transformations in the forecasts whenever one is used in the model definition. Common transformations include box-cox, logarithmic, and square root. The simplest specification of the model is 
`RW_model <- targets_use %>% model(RW = RW(observation))` which stores the model table (a `mable`) in the object called `RW_model`.
* Forecast: Then using this model, we run a forecast! We can specify whether bootstrapping is used and the number of bootstraps (`bootstrap = T`). 

Within this function, there are also if statements to test whether there are whole datasets missing, as well as messages which can be turned on/off with the `verbose = ` argument. 

 
```{r warning=FALSE, message =FALSE}
# Function carry out a random walk forecast
RW_daily_forecast <- function(site, 
                              var,
                              h,
                              bootstrap = FALSE, boot_number = 200,
                              transformation = 'none', verbose = TRUE,...) {
  
  # message('starting ',site_var_combinations$site[i], ' ', site_var_combinations$var[i], ' forecast')
  
  ### TIDY
  # filter the targets data set to the site_var pair
  targets_use <- targets %>%
    dplyr::filter(site_id == site,
           variable == var) %>%
    tsibble::as_tsibble(key = c('variable', 'site_id'), index = 'datetime') %>%
    
    # add NA values up to today (index)
    tsibble::fill_gaps(.end = Sys.Date()) %>%
    # Remove the NA's put at the end, so that the forecast starts from the last day with an observation,
    dplyr::filter(datetime < forecast_starts$reference_datetime[which(forecast_starts$site_id == site &
                                                                forecast_starts$variable == var)])
  


  # SPECIFY 
  
    # Do you want to apply a transformation? 
    if (transformation == 'log') {
      RW_model <- targets_use %>%
        model(RW = fable::RW(log(observation)))
    }
    
    if (transformation == 'log1p') {
      RW_model <- targets_use %>%
        model(RW = fable::RW(log1p(observation)))
    }
    
    if (transformation == 'sqrt') {
      RW_model <- targets_use %>%
        model(RW = fable::RW(sqrt(observation)))
    }
    
    if (transformation == 'none') {
      RW_model <- targets_use %>%
        model(RW = fable::RW(observation))
    }
    
    #FORECAST
    # Do you want to do a bootstrapped forecast?
    if (bootstrap == T) {
      forecast <- RW_model %>% 
        generate(h = as.numeric(forecast_starts$h[which(forecast_starts$site_id == site &
                                                          forecast_starts$variable == var)]),
                             bootstrap = T,
                             times = boot_number)
    }  else
      forecast <- RW_model %>% 
        forecast(h = as.numeric(forecast_starts$h[which(forecast_starts$site_id == site &
                                                                      forecast_starts$variable == var)]))
    
  if (verbose == T) {
    message(
      site,
      ' ',
      var,
      ' forecast with transformation = ',
      transformation,
      ' and bootstrap = ',
      bootstrap
    )
  }
    return(forecast)
    
  
}

```

This function takes just one site and one variable as arguments. To run across all site_id-variable combinations we can use a `for` loop. We need a data frame that we can index from.
The number of bootstraps (`boot_number`) is set to 200. It might also be useful to apply a transformation to some variables a `log()` transformation on the oxygen and chlorophyll values. 


We can then loop through each variable and site (row) and combine them into a single data frame (`RW_forecasts`).
```{r message = F}
site_var_combinations <- forecast_starts |> 
  select(site_id, variable) |> 
  rename(site = site_id,
         var = variable) |> 
  
  # assign the transformation depending on the variable. 
  # For example chla and oxygen might require a log(x ) transformation
  mutate(transformation = ifelse(var %in% c('chla', 'oxygen'), 
                                 'log', 
                                 'none')) 
head(site_var_combinations)
```


```{r }
# An empty data frame to put the forecasts in to
RW_forecast <- NULL

# Loop through each row (variable-site combination)
for (i in 1:nrow(site_var_combinations)) {
  
  forecast <- RW_daily_forecast(site = site_var_combinations$site[i],
                                var = site_var_combinations$var[i],
                                boot_number = 200,
                                bootstrap = T,
                                h = 30, 
                                verbose = F,
                                transformation = site_var_combinations$transformation[i])
  
  
  RW_forecast <- bind_rows(RW_forecast, forecast)
  
}

```



The output from the `forecast()` function is a forecast table or `fable`, which has columns for `variable`, `site_id`, the `.model`, the bootstrap value (1 to 200, `.rep`), and the prediction (`.sim`). 
```{r}
RW_forecast %>%
  filter(site_id == 'LEWI')
```



How reasonable are these forecasts?? Is there a way to improve the persistence model? Is a transformation needed?

```{r, message = F, warning = F, echo = F,fig.dim=c(5,25), fig.cap = "Figure: 'random walk' persistence forecasts for NEON lake sites"}
RW_forecast %>% 
  filter(site_id %in% c('ARIK', 'BLDE',
                        'CUPE', 'HOPB',
                        'KING', 'LECO', 
                        'LEWI', 'MART', 
                        'MAYF', 'MCDI',
                        'MCRA', 'OKSR'),
         variable == 'oxygen') %>%
  ggplot(.,aes(x=datetime, y=.sim, group = .rep)) + geom_line(alpha = 0.4) + 
  geom_point(data = subset(targets, site_id == 'LEWI'),
            aes(x=datetime, y=observation, group = 'obs'), colour = 'black') +
  facet_grid(variable~site_id, scales = 'free') + 
  theme_bw() +
  geom_point(data = subset(targets, site_id %in% c('ARIK', 'BLDE',
                        'CUPE', 'HOPB',
                        'KING', 'LECO', 
                        'LEWI', 'MART', 
                        'MAYF', 'MCDI',
                        'MCRA', 'OKSR') & variable == 'oxygen' ),
             aes(x=datetime, y=observation, group = 'obs'), colour = 'black') +
  theme(legend.position = 'none') +
  coord_cartesian(xlim = c(min(forecast_starts$reference_datetime[which(forecast_starts$site_id == 'LEWI')]) - 10,
                           Sys.Date() + 30)) +
  scale_x_date(expand = c(0,0), date_labels = "%d %b") +
  labs(y = 'value') +
  geom_vline(xintercept = Sys.Date(), linetype = 'dashed')
```

```{r echo=F, fig.dim=c(15,15), fig.cap='annotated forecasts', warning=FALSE}
RW_forecast %>% 
  filter(site_id %in% c('ARIK', 'BLDE',
                        'CUPE', 'HOPB',
                        'KING', 'LECO', 
                        'LEWI', 'MART', 
                        'MAYF', 'MCDI',
                        'MCRA', 'OKSR'),
         variable == 'oxygen') %>%
  ggplot(.,aes(x=datetime, y=.sim, group = .rep)) + 
  geom_point(data = subset(targets, site_id %in% c('ARIK', 'BLDE',
                        'CUPE', 'HOPB',
                        'KING', 'LECO', 
                        'LEWI', 'MART', 
                        'MAYF', 'MCDI',
                        'MCRA', 'OKSR') & variable == 'oxygen'),
             aes(x=datetime, y=observation, group = 'obs'), colour = 'darkblue') +
  geom_line(alpha = 0.3, aes(colour = 'ensemble member (parameter\n1 - 200)')) + 
  facet_wrap(~site_id, scales = 'free_y') +
  scale_x_date(expand = c(0,0), date_labels = "%d %b") +
  labs(y = 'value') +
  geom_vline(aes(linetype = 'reference_datetime', xintercept = Sys.Date()), colour = 'blue', size = 1.5) +
  labs(title = 'site_id', subtitle = 'variable = oxygen', caption = 'prediction') + 
  annotate("text", x = Sys.Date() - days(10), y = 20, label = "past")  +
  annotate("text", x = Sys.Date() + days(12), y = 20, label = "future")  +
  theme_bw() +
  coord_cartesian(xlim = c(max(forecast_starts$reference_datetime) - 15,
                           Sys.Date() + 30)) +
  scale_linetype_manual(values = 'dashed', name = '') +
  scale_colour_manual(values = 'darkgrey', name = '') +
  theme(strip.text = element_text(colour = 'orange'),
        axis.title.y = element_text(colour = 'green'),
        axis.title.x = element_text(colour = 'red'),
        axis.text.y = element_text(colour = 'purple'),
        axis.text.x = element_text(colour = 'red'),
        plot.caption = element_text(hjust = 0, colour = 'purple'),
        plot.title = element_text(colour = 'orange'), 
        plot.subtitle = element_text(colour = 'green')) 
```




## Model 3: Climatology model

An alternative approach is to look at the historic data to make predictions about the future. The seasonal naive model in `fable` sets each forecast to be equal to the last observed value, given the specified lag. When we specify the lag to by 1 year, it will provide a forecast that is the observations from the same day the previous year, also known as a climatology model. 
Again we need to tidy the data to the correct format for `fable`. We make sure there are explicit gaps (using `fill_gaps()`) and make it into a tsibble object with `variable` and `site_id` as the keys and `datetime` as the index. 
Then the `SNAIVE` model is fit with a 1 year lag.  One useful thing that the fable package can do is that it fits the specified models to each `key` pairing (variable, site_id) so you don't have to specify each model for each site and variable separately (we did not do this in Model 2 because each site and variable has a different date of last observation). 

```{r, warning = F}
SN_model <- targets %>%
  as_tsibble(key = c('variable', 'site_id'), index = 'datetime') %>%
  # add NA values up to today (index)
  fill_gaps(.end = Sys.Date()) %>%
  
  # Here we fit the model
  model(SN = SNAIVE(observation ~ lag('1 year')))
```

Then we use the model we've specified to forecast. `h = 30` specifies the horizon of the forecast, relative to the index of the data (as 30 days). If the index, in this case `datetime`, had a different value such as monthly, the `h = ` value would be months. We use `forecast(... , bootstrap = F)` to run a non-bootstrapped forecast. The forecast will run for each key combination (variable-site_id). When bootstrap = F, the model assumes a normal distribution. This would be an example of a distributional forecast that can be submitted to the Challenge.

```{r, warning=FALSE, error=FALSE, message=FALSE}
SN_forecast <- SN_model %>% forecast(h = 30, bootstrap = F)
SN_forecast
```

```{r}
convert_to_efi_standard <- function(df, model_id){
  ## determine variable name
  var <- attributes(df)$dist
  ## Normal distribution: use distribution mean and variance
  df %>% 
    dplyr::mutate(sigma = sqrt( distributional::variance( .data[[var]] ) ) ) %>%
    dplyr::rename(mu = .mean) %>%
    dplyr::select(datetime, site_id, .model, mu, sigma) %>%
    tidyr::pivot_longer(c(mu, sigma), names_to = "parameter", values_to = var) %>%
    dplyr::rename('prediction' = var) %>%
    mutate(family = "normal",
           reference_datetime = min(datetime) - lubridate::days(1),
           model_id = model_id, 
           variable = 'oxygen') %>%
    select(any_of(c('model_id', 'datetime', 'reference_datetime', 
                    'site_id', 'family', 'parameter', 'variable', 'prediction')))
}

```

```{r warning = FALSE}
model_name <- 'StreamDO_climatology'
SN_forecast_EFI <- convert_to_efi_standard(SN_forecast, 
                                           model_id = model_name)
```

```{r echo = FALSE}
SN_forecast_EFI
```


```{r, message = F, warning = F, echo = F, fig.cap="Figure: 'seasonal naive' forecasts for lake sites. Shade area show 95% confidence intervals"}
SN_forecast_EFI %>% 
  # filter(site_id == 'SUGG') %>%
  pivot_wider(names_from = 'parameter', values_from = 'prediction') %>%
  ggplot(.,aes(x=datetime)) + 
  geom_ribbon(aes(ymax = mu + (1.96*sigma),
                  ymin = mu - (1.96*sigma)), alpha = 0.2, fill = 'blue') +
  geom_line(aes(y = mu)) + 
  facet_grid(variable~site_id, scales = 'free') + 
  theme_bw() +
  coord_cartesian(xlim = c(Sys.Date() - 10,
                           Sys.Date() + 30)) +
  scale_x_date(expand = c(0,0), date_labels = "%d %b") +
  labs(y = 'value') +
  geom_vline(xintercept = Sys.Date(), linetype = 'dashed') +
  geom_point(data = targets, aes(x=datetime, y= observation))
```